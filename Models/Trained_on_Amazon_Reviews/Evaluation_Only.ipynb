{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e65316b1",
   "metadata": {
    "id": "e65316b1"
   },
   "source": [
    "# Comparing Word Embeddings for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1071e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import json\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, FastText, KeyedVectors\n",
    "import os\n",
    "\n",
    "# Get the current working directory and define the base directory.\n",
    "current_dir = os.getcwd()\n",
    "# get rid of two level of the directory\n",
    "base_dir = os.path.dirname(current_dir)\n",
    "base_dir = os.path.dirname(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "804104e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained models:\n",
    "word2vec_model = Word2Vec.load('Models/word2vec.model')\n",
    "fasttext_model = FastText.load('Models/fasttext.model')\n",
    "glove_model = KeyedVectors.load_word2vec_format('Models/glove/vectors.txt', binary=False, no_header=True)\n",
    "\n",
    "# Define seed words for sentiment\n",
    "positive_words = [\"good\", \"great\", \"excellent\", \"positive\", \"fortunate\", \"correct\", \"superior\", \"happy\", \"beneficial\"]\n",
    "negative_words = [\"bad\", \"terrible\", \"poor\", \"negative\", \"unfortunate\", \"wrong\", \"inferior\", \"sad\", \"harmful\"]\n",
    "\n",
    "def compute_average_vector(words, model):\n",
    "    \"\"\"Compute the average vector for a list of words from the given model.\"\"\"\n",
    "    vectors = []\n",
    "    # For Word2Vec and FastText models, use .wv; for GloVe loaded as KeyedVectors, use the model directly.\n",
    "    for word in words:\n",
    "        if hasattr(model, 'wv'):\n",
    "            if word in model.wv:\n",
    "                vectors.append(model.wv[word])\n",
    "        else:\n",
    "            if word in model:\n",
    "                vectors.append(model[word])\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Precompute seed vectors for each model\n",
    "pos_vec_word2vec = compute_average_vector(positive_words, word2vec_model)\n",
    "neg_vec_word2vec = compute_average_vector(negative_words, word2vec_model)\n",
    "\n",
    "pos_vec_fasttext = compute_average_vector(positive_words, fasttext_model)\n",
    "neg_vec_fasttext = compute_average_vector(negative_words, fasttext_model)\n",
    "\n",
    "pos_vec_glove = compute_average_vector(positive_words, glove_model)\n",
    "neg_vec_glove = compute_average_vector(negative_words, glove_model)\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"Compute the cosine similarity between two vectors.\"\"\"\n",
    "    if v1 is None or v2 is None:\n",
    "        return 0\n",
    "    norm1 = np.linalg.norm(v1)\n",
    "    norm2 = np.linalg.norm(v2)\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0\n",
    "    return np.dot(v1, v2) / (norm1 * norm2)\n",
    "\n",
    "def get_sentence_vector(text, model):\n",
    "    \"\"\"Tokenize the text and compute the average embedding vector using the provided model.\"\"\"\n",
    "    # Simple tokenization (you can improve this with nltk or regex as needed)\n",
    "    words = text.lower().split()\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        # Remove common punctuation\n",
    "        word = word.strip('.,!?\";:')\n",
    "        if hasattr(model, 'wv'):\n",
    "            if word in model.wv:\n",
    "                vectors.append(model.wv[word])\n",
    "        else:\n",
    "            if word in model:\n",
    "                vectors.append(model[word])\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def sentiment_score(text, model, pos_seed, neg_seed):\n",
    "    \"\"\"\n",
    "    Calculate a simple sentiment score for the given text using the provided model.\n",
    "    The score is defined as the cosine similarity between the text vector and the positive seed vector\n",
    "    minus the similarity between the text vector and the negative seed vector.\n",
    "    \"\"\"\n",
    "    sent_vec = get_sentence_vector(text, model)\n",
    "    if sent_vec is None:\n",
    "        return 0\n",
    "    pos_sim = cosine_similarity(sent_vec, pos_seed)\n",
    "    neg_sim = cosine_similarity(sent_vec, neg_seed)\n",
    "    return pos_sim - neg_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42ce9cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sentiment Analysis Summary for NYT Articles ---\n",
      "word2vec - Average Score: -0.1353 | Positive: 1 | Negative: 9 | Total Articles: 10\n",
      "fasttext - Average Score: -0.0355 | Positive: 4 | Negative: 6 | Total Articles: 10\n",
      "glove - Average Score: 0.0250 | Positive: 6 | Negative: 4 | Total Articles: 10\n",
      "\n",
      "\n",
      "--- Sentiment Analysis Summary for US News Articles ---\n",
      "word2vec - Average Score: -0.1077 | Positive: 3 | Negative: 6 | Total Articles: 9\n",
      "fasttext - Average Score: -0.0752 | Positive: 1 | Negative: 8 | Total Articles: 9\n",
      "glove - Average Score: 0.0319 | Positive: 5 | Negative: 4 | Total Articles: 9\n",
      "\n",
      "\n",
      "--- Sentiment Analysis Summary for WP Articles ---\n",
      "word2vec - Average Score: -0.1035 | Positive: 5 | Negative: 11 | Total Articles: 16\n",
      "fasttext - Average Score: -0.0387 | Positive: 7 | Negative: 9 | Total Articles: 16\n",
      "glove - Average Score: 0.0256 | Positive: 9 | Negative: 7 | Total Articles: 16\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_dataset(dataset_file, dataset_name):\n",
    "    \"\"\"Evaluate sentiment on all articles in the given dataset and print summary statistics.\"\"\"\n",
    "    with open(dataset_file, 'r', encoding='utf-8') as f:\n",
    "        articles = json.load(f)\n",
    "    \n",
    "    results = {\n",
    "        \"word2vec\": [],\n",
    "        \"fasttext\": [],\n",
    "        \"glove\": []\n",
    "    }\n",
    "    \n",
    "    # Process each article.\n",
    "    for article in articles:\n",
    "        title = article.get(\"title\", \"\")\n",
    "        content = article.get(\"content\", \"\")\n",
    "        full_text = title + \" \" + content\n",
    "        \n",
    "        score_w2v = sentiment_score(full_text, word2vec_model, pos_vec_word2vec, neg_vec_word2vec)\n",
    "        score_fasttext = sentiment_score(full_text, fasttext_model, pos_vec_fasttext, neg_vec_fasttext)\n",
    "        score_glove = sentiment_score(full_text, glove_model, pos_vec_glove, neg_vec_glove)\n",
    "        \n",
    "        results[\"word2vec\"].append(score_w2v)\n",
    "        results[\"fasttext\"].append(score_fasttext)\n",
    "        results[\"glove\"].append(score_glove)\n",
    "    \n",
    "    # Compute and print summary statistics.\n",
    "    print(f\"--- Sentiment Analysis Summary for {dataset_name} ---\")\n",
    "    for model_name, scores in results.items():\n",
    "        if scores:\n",
    "            avg_score = np.mean(scores)\n",
    "            positive_count = sum(1 for s in scores if s > 0)\n",
    "            negative_count = sum(1 for s in scores if s < 0)\n",
    "            total = len(scores)\n",
    "            print(f\"{model_name} - Average Score: {avg_score:.4f} | Positive: {positive_count} | Negative: {negative_count} | Total Articles: {total}\")\n",
    "        else:\n",
    "            print(f\"{model_name} - No articles processed.\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Build dataset paths relative to the base directory.\n",
    "datasets = [\n",
    "    (os.path.join(base_dir, \"Training Data\", \"News Articles\", \"Output\", \"data_nyt.json\"), \"NYT Articles\"),\n",
    "    (os.path.join(base_dir, \"Training Data\", \"News Articles\", \"Output\", \"data_usn.json\"), \"US News Articles\"),\n",
    "    (os.path.join(base_dir, \"Training Data\", \"News Articles\", \"Output\", \"data_wp.json\"), \"WP Articles\")\n",
    "]\n",
    "\n",
    "# Evaluate sentiment on each dataset.\n",
    "for file_path, name in datasets:\n",
    "    evaluate_dataset(file_path, name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
